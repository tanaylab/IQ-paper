#!/usr/bin/env python
"""Train a CREsted model for the IQ-paper benchmarks (Figure 5)."""

import os
# Set backend
os.environ["KERAS_BACKEND"] = "torch"
import argparse
from pathlib import Path
import numpy as np
import crested


def parse_args():
    parser = argparse.ArgumentParser(description="Train CRESTED model on genomic data")
    parser.add_argument("--bigwigs-folder", required=True, help="Path to folder containing bigwig files")
    parser.add_argument("--regions-file", required=True, help="Path to regions BED file")
    parser.add_argument("--output-dir", required=True, help="Output directory for results")
    parser.add_argument("--project-name", required=True, help="Project name for logging")
    parser.add_argument(
        "--split-strategy", choices=["chr", "region"], default="chr", help="Strategy for train/val/test split: 'chr' or 'region'",
    )
    parser.add_argument("--val-size", type=float, default=0.1, help="Validation set size (only if split_strategy='region')")
    parser.add_argument("--test-size", type=float, default=0.1, help="Test set size (only if split_strategy='region')")
    parser.add_argument("--val-chroms", nargs="+", default=["chr8", "chr10"], help="Validation chromosomes (split_strategy='chr')")
    parser.add_argument("--test-chroms", nargs="+", default=["chr9", "chr18"], help="Test chromosomes (split_strategy='chr')")
    parser.add_argument("--batch-size", type=int, default=128, help="Training batch size")
    parser.add_argument("--epochs", type=int, default=60, help="Number of training epochs")
    parser.add_argument("--logger", choices=["wandb", "tensorboard"], default="wandb", help="Logger to use for training")
    parser.add_argument(
        "--architecture",
        choices=[
            "basenji",
            "borzoi",
            "chrombpnet",
            "chrombpnet_decoupled",
            "deeptopic_cnn",
            "deeptopic_lstm",
            "enformer",
            "simple_convnet",
        ],
        default="chrombpnet",
        help="Model architecture to use",
    )
    parser.add_argument("--learning-rate", type=float, default=None, help="Learning rate (overrides defaults)")
    parser.add_argument("--custom-seq-len", type=int, default=None, help="Custom sequence length")

    parser.add_argument("--chromsizes-file", default="mm10.chrom.sizes", help="Chromosome sizes file")
    parser.add_argument("--genome-file", default="mm10.fa", help="Genome FASTA file")

    return parser.parse_args()


def write_run_script(args, output_dir):
    """Save a bash one-liner that exactly reproduces this run."""
    script_path = output_dir / "reproduce_run.sh"
    python_script = os.path.abspath(__file__)

    command_parts = [
        "#!/bin/bash",
        "",
        "# Auto-generated by train_crested.py",
        "",
        f"python {python_script} \\",
    ]

    required_args = [
        ("bigwigs-folder", args.bigwigs_folder),
        ("regions-file", args.regions_file),
        ("output-dir", args.output_dir),
        ("project-name", args.project_name),
        ("split-strategy", args.split_strategy),
        ("logger", args.logger),
        ("batch-size", args.batch_size),
        ("epochs", args.epochs),
        ("architecture", args.architecture),
    ]

    for name, val in required_args:
        command_parts.append(f"    --{name} {val} \\")

    if args.learning_rate is not None:
        command_parts.append(f"    --learning-rate {args.learning_rate} \\")
    if args.custom_seq_len is not None:
        command_parts.append(f"    --custom-seq-len {args.custom_seq_len} \\")

    if args.split_strategy == "region":
        command_parts.extend([f"    --val-size {args.val_size} \\", f"    --test-size {args.test_size} \\"])
    else:
        command_parts.extend(
            [
                f"    --val-chroms {' '.join(args.val_chroms)} \\",
                f"    --test-chroms {' '.join(args.test_chroms)} \\",
            ]
        )

    default_chromsizes = "mm10.chrom.sizes"
    default_genome = "mm10.fa"

    if args.chromsizes_file != default_chromsizes:
        command_parts.append(f"    --chromsizes-file {args.chromsizes_file} \\")
    if args.genome_file != default_genome:
        command_parts.append(f"    --genome-file {args.genome_file} \\")

    command_parts[-1] = command_parts[-1].rstrip(" \\")  # remove trailing backslash

    with open(script_path, "w") as fh:
        fh.write("\n".join(command_parts))
    os.chmod(script_path, 0o755)


def main():
    args = parse_args()

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    run_name = output_dir.name

    architecture_widths = {
        "basenji": 2114,
        "borzoi": 2048,  # must be divisible by 128
        "chrombpnet": 2114,
        "chrombpnet_decoupled": 2114,
        "deeptopic_cnn": 500,
        "deeptopic_lstm": 500,
        "enformer": 2114,
        "simple_convnet": 2114,
    }

    seq_len = args.custom_seq_len or architecture_widths.get(args.architecture, 2114)
    print(f"Using sequence length: {seq_len} for architecture: {args.architecture}")

    write_run_script(args, output_dir)

    # ---------------------------------------------------------------------
    # Data import & preprocessing
    # ---------------------------------------------------------------------
    adata = crested.import_bigwigs(
        bigwigs_folder=args.bigwigs_folder,
        regions_file=args.regions_file,
        target_region_width=500,
        chromsizes_file=args.chromsizes_file,
        target="mean",
    )

    if args.split_strategy == "chr":
        crested.pp.train_val_test_split(
            adata,
            strategy="chr",
            val_chroms=args.val_chroms,
            test_chroms=args.test_chroms,
        )
    else:
        crested.pp.train_val_test_split(
            adata,
            strategy="region",
            val_size=args.val_size,
            test_size=args.test_size,
            random_state=42,
        )

    print(adata.var["split"].value_counts())

    crested.pp.change_regions_width(adata, seq_len, chromsizes_file=args.chromsizes_file)
    crested.pp.normalize_peaks(adata, top_k_percent=0.03)
    adata.write_h5ad(output_dir / "preprocessed_data.h5ad")

    datamodule = crested.tl.data.AnnDataModule(
        adata,
        genome=args.genome_file,
        batch_size=args.batch_size,
        max_stochastic_shift=3,
        always_reverse_complement=True,
    )

    architecture_map = {
        "basenji": crested.tl.zoo.basenji,
        "borzoi": crested.tl.zoo.borzoi,
        "chrombpnet": crested.tl.zoo.chrombpnet,
        "chrombpnet_decoupled": crested.tl.zoo.chrombpnet_decoupled,
        "deeptopic_cnn": crested.tl.zoo.deeptopic_cnn,
        "deeptopic_lstm": crested.tl.zoo.deeptopic_lstm,
        "enformer": crested.tl.zoo.enformer,
        "simple_convnet": crested.tl.zoo.simple_convnet,
    }

    model_architecture = architecture_map[args.architecture](seq_len=seq_len, num_classes=len(list(adata.obs_names)))

    if args.learning_rate is None:
        if args.architecture == "deeptopic_lstm":
            learning_rate = 1e-5
        elif args.architecture == "enformer":
            learning_rate = 5e-5
        else:
            learning_rate = 1e-3
    else:
        learning_rate = args.learning_rate

    print(f"Using learning rate: {learning_rate} for architecture: {args.architecture}")

    import keras

    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
    loss = crested.tl.losses.CosineMSELogLoss(max_weight=100)
    metrics = [
        keras.metrics.MeanAbsoluteError(),
        keras.metrics.MeanSquaredError(),
        keras.metrics.CosineSimilarity(axis=1),
        crested.tl.metrics.PearsonCorrelation(),
        crested.tl.metrics.ConcordanceCorrelationCoefficient(),
        crested.tl.metrics.PearsonCorrelationLog(),
        crested.tl.metrics.ZeroPenaltyMetric(),
    ]

    config = crested.tl.TaskConfig(optimizer, loss, metrics)

    os.environ["KERAS_BACKEND"] = "torch"
    trainer = crested.tl.Crested(
        data=datamodule,
        model=model_architecture,
        config=config,
        project_name=args.project_name,
        run_name=run_name,
        logger=args.logger,
    )

    os.environ["KERAS_BACKEND"] = "tensorflow"
    trainer.fit(epochs=args.epochs, early_stopping=True)
    trainer.test()
    trainer.predict(adata, model_name=run_name)

    adata.write_h5ad(f"{output_dir}/final_data_with_predictions.h5ad")

    test_index = adata.var["split"] == "test"
    correlations = []
    with open(f"{output_dir}/test_correlations.txt", "w") as fh:
        for traj in adata.obs_names:
            true = adata.X[adata.obs_names == traj, test_index]
            pred = adata.layers[run_name][adata.obs_names == traj, test_index]
            r2 = np.corrcoef(true.flatten(), pred.flatten())[0, 1] ** 2
            fh.write(f"{traj} r^2: {r2}\n")
            correlations.append(r2)
        fh.write(f"\nMean r^2: {np.mean(correlations)}")


if __name__ == "__main__":
    main() 